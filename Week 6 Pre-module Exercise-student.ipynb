{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 6: Model Training\n",
    "# Pre-module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to Class Imbalance\n",
    "\n",
    "In this week's pre-module, we will explore an important concept in data preprocessing: **class imbalance**. This issue frequently arises in biological datasets — particularly when studying rare diseases — where one class (e.g., healthy individuals) vastly outnumbers the other (e.g., patients with a rare condition). To better understand this concept, let's begin with an illustrative analogy.\n",
    "\n",
    "**The Dragon Analogy:**\n",
    "\n",
    "Imagine you're a knight in a mythical land, tasked with a crucial mission: to identify dragons among a vast array of animals. According to legend, collecting enough dragon scales grants immortality. However, there's a challenge – dragons are incredibly rare, and until recently, no one had ever seen one. Recently, two dragons have been captured, providing you with your first real glimpse of these elusive creatures. Armed with this limited knowledge, you set out on your quest to classify animals as either **dragons** or **not dragons** – a classic **binary classification** problem. But with only two dragons ever seen, how can you confidently distinguish dragons from other animals, especially when other animals vary widely in appearance?\n",
    "\n",
    "Upon returning from your journey, you need to report your findings to the king. A simple way to measure your success might be to report your overall accuracy—how many dragons you correctly identified. But here’s the catch: because dragons are so rare, you could label every animal as “not a dragon” and still achieve 99.9% accuracy – after all, we know that dragons are exceedingly rare. That sounds impressive, but misses the entire point of your mission: finding the dragons, the rare 0.01%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **The Challenge of Class Imbalance**\n",
    "This analogy illustrates a key issue in machine learning: <span style=\"background-color: #AFEEEE\">**class imbalance**</span>. When one class (e.g., dragons) is significantly underrepresented compared to another (e.g., non-dragons), traditional metrics like accuracy can be misleading. A model trained on such imbalanced data may fail to learn the distinguishing features of the minority class. If the training set only has two dragons, the model lacks enough information to learn what makes dragons unique. If the test set is also imbalanced, accuracy alone won’t reflect how well the model identifies the rare class. In such cases, alternative evaluation metrics like precision-recall and the F1 score are more informative. Additionally, we can use techniques such as resampling or apply specialized algorithms to better handle the imbalance.\n",
    "\n",
    "#### **Class Imbalance in Biological Data**\n",
    "In biological contexts, class imbalance is common. For example, when analyzing years of chest X-rays from a hospital, most images will likely show healthy lungs, while only a few may reveal pneumonia. In this case, the healthy images form the <span style=\"background-color: #AFEEEE\">**majority class**</span>, and the pneumonia cases form the <span style=\"background-color: #AFEEEE\">**minority class**</span>.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Exploring Our Dataset**\n",
    "\n",
    "Before we explore various techniques to address class imbalance, let's first examine the class distribution within our breast cancer dataset. This analysis will provide a clear understanding of how class imbalance manifests in real-world data and set the stage for the strategies we’ll explore next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install these necessary packages\n",
    "!pip install -q imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "df = pd.read_csv('bc_data.csv', index_col=0)\n",
    "\n",
    "B,M = df.diagnosis.value_counts()\n",
    "plt.pie([B,M], labels = [\"B\",\"M\"], colors = sns.color_palette('bright')[0:2], autopct='%.0f%%')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our analysis, we can see that the benign class is significantly larger than the malignant class; thus, making the malignant class the minority class makes sense from a clinical perspective."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handling class imbalance is crucial in training effective machine learning models, especially in binary classification problems, which are frequently tackled using methods like logistic regression. The issue of imbalanced datasets is particularly problematic because it can lead to models that are inherently biased towards the majority class. This bias occurs for several reasons:\n",
    "\n",
    "<span style=\"background-color: #AFEEEE\">**Learning from Frequency:**</span> Machine learning algorithms often learn by identifying patterns that are most frequent in the dataset. In an imbalanced dataset, the majority class dominates these patterns, leading the model to develop a strong bias towards it. This is because the algorithm has more examples from the majority class to learn from, making it more familiar and thus more likely to predict an instance as belonging to this class.\n",
    "\n",
    "<span style=\"background-color: #AFEEEE\">**Error Minimization Strategy:**</span> Most standard machine learning algorithms aim to minimize overall error. In a scenario where one class vastly outnumbers the other, the simplest way to achieve low error rates is by favoring the majority class. For instance, in a dataset where 95% of the samples are of the majority class, a model can achieve 95% accuracy by simply predicting every instance as the majority class, completely ignoring the minority class."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Penalized Models\n",
    "\n",
    "**Penalized models** introduce a concept of imposing greater penalties on the model for misclassifying instances of the minority class. This approach is particularly useful when dealing with imbalanced datasets. We can adjust class weights in logistic regression to make the model pay more attention to the minority class. **Adjusting class weights** is a technique that involves modifying the weights assigned to different classes. By increasing the weight, or importance, of the minority class within the algorithm's cost function, the model is encouraged to pay more attention to the minority class.\n",
    "\n",
    "This method is centered around recalibrating the model's focus to better address the imbalances. By altering the model's objective function, this approach shifts the model's attention towards more accurately predicting a particular class, in this case, instructing the model to prioritize accuracy in the minority class. This strategy is particularly relevant in scenarios where the minority class is of greater interest or importance, despite being underrepresented in the dataset.\n",
    "\n",
    "The effectiveness of this method is evaluated based on its ability to enhance the model's sensitivity to the minority class without compromising overall performance. While this approach can lead to a decrease in overall accuracy due to the model's increased focus on the minority class, the key benefit lies in improved detection of the underrepresented group. However, it's crucial to strike a balance to avoid overfitting the minority class, which can result in poor generalization and an increase in false positives.\n",
    "\n",
    "The implementation of this approach is straightforward in many logistic regression models, such as those provided by the `sklearn` library in Python. The `class_weight` parameter is used to specify the weights for each class. For instance, if the minority class is underrepresented by a factor of 3:1 compared to the majority class, you could assign a weight of 3 to the minority class and 1 to the majority class. This numerical adjustment tells the model to \"care\" three times as much about errors in the minority class as it does about the majority class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"background-color: #FFD700\">**Complete the code cell below**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "# TODO: Store the diagnosis column in the `target` variable, and the predictors in the `features`\n",
    "features = ...\n",
    "target = ...\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(target), y=target)\n",
    "class_weights_dict = dict(enumerate(class_weights))\n",
    "\n",
    "# Create a logistic regression model with class weights\n",
    "# TODO: Set the  `class_weight` parameter to the dictionary \n",
    "balanced_model = SGDClassifier(...)\n",
    "\n",
    "# Check the set class weights\n",
    "set_class_weights = balanced_model.class_weight\n",
    "\n",
    "print(\"Set Class Weights:\", set_class_weights)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Resampling Methods\n",
    "Resampling is a straightforward approach to adjust the balance of your classes.\n",
    "\n",
    "<span style=\"background-color: #AFEEEE\">- **Oversampling the Minority Class:**</span> This involves adding more copies of the minority class to the dataset. A sophisticated method of oversampling is Synthetic Minority Over-sampling Technique (SMOTE), which creates synthetic samples that are similar to the existing ones in the minority class.\n",
    "\n",
    "<span style=\"background-color: #AFEEEE\">- **Undersampling the Majority Class:**</span> This technique reduces the number of instances from the majority class to balance the dataset. Care must be taken to ensure that important information from the majority class is not lost.\n",
    "\n",
    "Below is an example of undersampling of our dataset. Please fill in the sections that are missing based on your established `pandas` knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"background-color: #FFD700\">**Complete the code cell below**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Undersampling\n",
    "# TODO: Separate the majority and minority classes\n",
    "df_majority = ...\n",
    "df_minority = ...\n",
    "\n",
    "# Downsample the majority class\n",
    "df_majority_downsampled = df_majority.sample(n=len(df_minority), random_state=16)\n",
    "\n",
    "# Combine minority class with downsampled majority class\n",
    "df_balanced = pd.concat([df_majority_downsampled, df_minority])\n",
    "\n",
    "# Shuffle the dataset to mix up the rows\n",
    "df_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Now df_balanced has a balanced target\n",
    "\n",
    "# Count the number of instances of each class\n",
    "class_counts = df_balanced['diagnosis'].value_counts()\n",
    "\n",
    "# Print the counts\n",
    "print(class_counts)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving and Loading Models\n",
    "\n",
    "Saving and loading trained models is a crucial aspect of machine learning, especially when you need to deploy models or use them for predictions later without having to retrain them. `Sklearn` models can be easily saved and loaded using Python's built-in `pickle` module. To learn more about the module, you may visit the documentation here: https://docs.python.org/3/library/pickle.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Using pickle**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT RUN THIS CODE!\n",
    "\n",
    "# Saving a Model with `pickle`:\n",
    "import pickle\n",
    "\n",
    "# Assuming 'model' is your trained sklearn model\n",
    "model = ...  # your trained model here\n",
    "\n",
    "# Save the model to a file\n",
    "with open('model.pkl', 'wb') as file:\n",
    "    pickle.dump(model, file)\n",
    "\n",
    "# This code will write the trained model to a file named 'model.pkl' in binary write mode (`'wb'`). The model can then be reloaded and used for predictions without the need to retrain.\n",
    "\n",
    "# Loading a Model with `pickle`:\n",
    "with open('model.pkl', 'rb') as file:\n",
    "    loaded_model = pickle.load(file)\n",
    "\n",
    "# Use the loaded model to make predictions\n",
    "# predictions = loaded_model.predict(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the model is read from 'model.pkl' in binary read mode (`'rb'`) and loaded into `loaded_model`, which can then be used for further predictions or analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Graded Exercise: (1 mark)** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GQ*1: Why is accuracy alone a misleading metric for evaluating model performance in imbalanced datasets like the \"dragon analogy\" scenario? Explain with reference to the majority/minority class in 1-2 sentences (1 mark).**\n",
    "\n",
    "<span style=\"background-color: #FFD700\">**Write your answer below**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer here:\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "The choice of technique depends on the specific context of the problem and the nature of the data. It's often useful to try multiple approaches and compare their performance using appropriate metrics. Understanding the trade-offs between different methods is key to effectively handling class imbalance in logistic regression and other machine learning models. Fortunately for us, our dataset is not very affected by class imbalance; however, it never hurts to try to balance the classes when possible.\n",
    "\n",
    "Moreover, we looked at how to save and load a model. Thus, the next time we want to use the same model between files or share our progress with others, it will be less of a hassle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
